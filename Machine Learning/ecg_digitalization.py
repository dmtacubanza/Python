# -*- coding: utf-8 -*-
"""ECG Digitalization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HhzywdDDADb8495qe-CoVmd6hZrHvmV2
"""

!pip install heartpy

import os
import heartpy as hp
from IPython.display import display
from scipy.signal import resample
from PIL import Image
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path

from __future__ import division
#from matplotlib import pyplot as plt
import scipy.io as spio
#import numpy as np
import statistics
from scipy.stats import kurtosis
from scipy.stats import skew
import sys
#sys.path.append("/home/chandan/python-workspace/")
#import BOCPD as ocpd #import bocpd from another file
import cProfile
from functools import partial

from scipy import signal
from scipy.ndimage import label as sci_label
from scipy.stats import zscore
from scipy.interpolate import interp1d
from scipy.integrate import trapz

def detect_peaks(ecg_signal, threshold=0.3, qrs_filter=None):
    '''
    Peak detection algorithm using cross corrrelation and threshold
    '''
    if qrs_filter is None:
        # create default qrs filter, which is just a part of the sine function
        t = np.linspace(1.5 * np.pi, 3.5 * np.pi, 15)
        qrs_filter = np.sin(t)

    # normalize data
    ecg_signal = (ecg_signal - ecg_signal.mean()) / ecg_signal.std()

    # calculate cross correlation
    similarity = np.correlate(ecg_signal, qrs_filter, mode="same")
    similarity = similarity / np.max(similarity)

    # return peaks (values in ms) using threshold
    #return ecg_signal[similarity > threshold].index, similarity
    return np.where(ecg_signal[similarity > threshold]), similarity
    #return ecg_signal.where(similarity > threshold), similarity

def group_peaks(p, threshold=5):
    '''
    The peak detection algorithm finds multiple peaks for each QRS complex.
    Here we group collections of peaks that are very near (within threshold) and we take the median index
    '''
    # initialize output
    output = np.empty(0)

    # label groups of sample that belong to the same peak
    peak_groups, num_groups = sci_label(np.diff(p) < threshold)

    # iterate through groups and take the mean as peak index
    for i in np.unique(peak_groups)[1:]:
    #for i in np.unique(peak_groups):
        peak_group = p[np.where(peak_groups == i)]
        output = np.append(output, np.median(peak_group))
    return output

"""TIME DOMAIN"""
#independent function to calculate RMSSD
def calc_rmssd(list):
    diff_nni = np.diff(list)#successive differences
    return np.sqrt(np.mean(diff_nni ** 2))


 #independent function to calculate AVRR
def calc_avrr(list):
    return sum(list)/len(list)

 #independent function to calculate SDRR
def calc_sdrr(list):
    return statistics.stdev(list)

 #independent function to calculate SKEW
def calc_skew(list):
    return skew(list)

 #independent function to calculate KURT
def calc_kurt(list):
    return kurtosis(list)

def calc_NNx(list):
    #diff_nni = np.diff(list)
    # detect peaks
    peaks, similarity = detect_peaks(list, threshold=0.3)
    # group peaks so we get a single peak per beat (hopefully)
    grouped_peaks = group_peaks(peaks)
    # RR-intervals are the differences between successive peaks
    rr = np.diff(grouped_peaks)
    nnxx = np.sum(np.abs(np.diff(rr)) > 50)*1
    #return sum(np.abs(diff_nni) > 50)
    return nnxx

def calc_pNNx(list):
    #length_int = len(list)
    #diff_nni = np.diff(list)
    #nni_50 = sum(np.abs(diff_nni) > 50)
    #return 100 * nni_50 / length_int
    # detect peaks
    peaks, similarity = detect_peaks(list, threshold=0.3)
    # group peaks so we get a single peak per beat (hopefully)
    grouped_peaks = group_peaks(peaks)
    # RR-intervals are the differences between successive peaks
    rr = np.diff(grouped_peaks)
    pnnxx = 100 * np.sum((np.abs(np.diff(rr)) > 50)*1) / len(rr)
    #return sum(np.abs(diff_nni) > 50)
    return pnnxx

"""NON LINEAR DOMAIN"""
 #independent function to calculate SD1
def calc_SD1(list):
    diff_nn_intervals = np.diff(list)
    return np.sqrt(np.std(diff_nn_intervals, ddof=1) ** 2 * 0.5)
 #independent function to calculate SD2
def calc_SD2(list):
    diff_nn_intervals = np.diff(list)
    return np.sqrt(2 * np.std(list, ddof=1) ** 2 - 0.5 * np.std(\
                   diff_nn_intervals, ddof=1) ** 2)

 #independent function to calculate SD1/SD2
def calc_SD1overSD2(list):
      diff_nn_intervals = np.diff(list)
      sd1 = np.sqrt(np.std(diff_nn_intervals, ddof=1) ** 2 * 0.5)
      sd2 = np.sqrt(2 * np.std(list, ddof=1) ** 2 - 0.5 * np.std(\
                    diff_nn_intervals, ddof=1) ** 2)
      ratio_sd2_sd1 = sd2 / sd1
      return ratio_sd2_sd1


 #independent function to calculate CSI
def calc_CSI(list):
    diff_nn_intervals = np.diff(list)
    sd1 = np.sqrt(np.std(diff_nn_intervals, ddof=1) ** 2 * 0.5)
    sd2 = np.sqrt(2 * np.std(list, ddof=1) ** 2 - 0.5 * np.std(\
                  diff_nn_intervals, ddof=1) ** 2)
    L=4 * sd1
    T=4 * sd2
    return L/T

 #independent function to calculate CVI
def calc_CVI(list):
    diff_nn_intervals = np.diff(list)
    sd1 = np.sqrt(np.std(diff_nn_intervals, ddof=1) ** 2 * 0.5)
    sd2 = np.sqrt(2 * np.std(list, ddof=1) ** 2 - 0.5 * np.std(\
                  diff_nn_intervals, ddof=1) ** 2)
    L=4 * sd1
    T=4 * sd2
    return np.log10(L * T)

 #independent function to calculate modified CVI
def calc_modifiedCVI(list):
    diff_nn_intervals = np.diff(list)
    sd1 = np.sqrt(np.std(diff_nn_intervals, ddof=1) ** 2 * 0.5)
    sd2 = np.sqrt(2 * np.std(list, ddof=1) ** 2 - 0.5 * np.std(\
                  diff_nn_intervals, ddof=1) ** 2)
    L=4 * sd1
    T=4 * sd2
    return L ** 2 / T


def calc_meanrr(list):
    # detect peaks
    peaks, similarity = detect_peaks(list, threshold=0.3)
    # group peaks so we get a single peak per beat (hopefully)
    grouped_peaks = group_peaks(peaks)
    # RR-intervals are the differences between successive peaks
    rr = np.diff(grouped_peaks)
    return np.mean(rr)

def calc_medianrr(list):
    # detect peaks
    peaks, similarity = detect_peaks(list, threshold=0.3)
    # group peaks so we get a single peak per beat (hopefully)
    grouped_peaks = group_peaks(peaks)
    # RR-intervals are the differences between successive peaks
    rr = np.diff(grouped_peaks)
    return np.median(rr)


def calc_hr(list):
    # detect peaks
    peaks, similarity = detect_peaks(list, threshold=0.3)
    # group peaks so we get a single peak per beat (hopefully)
    grouped_peaks = group_peaks(peaks)
    # RR-intervals are the differences between successive peaks
    rr = np.diff(grouped_peaks)
    hr = 60000/rr
    return np.mean(hr)

rows = []
emotion = None
valid = False
name = None
state = {
    "R": "Relaxed",
    "F": "Frustrated",
    "S": "Satisfied",
    }

for ecg_pdf_filename in os.listdir('/content'):
  valid = False
  if ecg_pdf_filename.endswith("png"):
    im = Image.open(ecg_pdf_filename)
    name = ecg_pdf_filename
    emotion = ecg_pdf_filename.split("_")[2]
    emotion = emotion.split("-")[0]
    emotion  = emotion.split(".")[0]
    im_arr = np.array(im)
    im_arr.shape

    #plt.figure(figsize=(20,5))
    #plt.imshow(im_arr[453:633,83:-90]);

    arr_0 = im_arr[453:633,83:-90]
    arr_1 = im_arr[671:851,83:-90]
    arr_2 = im_arr[889:1069,83:-90]


    #plt.figure(figsize=(20,20))
    #plt.imshow(arr_0, cmap='Blues');
    #plt.figure(figsize=(20,20))
    #plt.imshow(arr_1, cmap='Blues');
    #plt.figure(figsize=(20,20))
    #plt.imshow(arr_2, cmap='Blues');


    #pull out waveform using argmin (subtract from 180 as signal will be flipped)
    recreated_ecg_arr = 180-np.hstack([arr_0, arr_1, arr_2]).argmin(axis=0)

    skew_ = calc_skew(recreated_ecg_arr)
    kurt = calc_kurt(recreated_ecg_arr)
    avrr = calc_avrr(recreated_ecg_arr)
    sdrr = calc_sdrr(recreated_ecg_arr)
    csi = calc_CSI(recreated_ecg_arr)
    cvi = calc_CVI(recreated_ecg_arr)
    modifiedcvi = calc_modifiedCVI(recreated_ecg_arr)
    meanrr = calc_meanrr(recreated_ecg_arr)
    medianrr = calc_medianrr(recreated_ecg_arr)
    hr = calc_hr(recreated_ecg_arr)
    sdrr_rmssd = calc_sdrr(recreated_ecg_arr) / calc_rmssd(recreated_ecg_arr)
    #sdrr_rmssd = sdrrrmssd

    #print(recreated_ecg_arr)
    recreated_ecg = (pd.DataFrame(recreated_ecg_arr,
                             pd.timedelta_range('0S', '30S', len(recreated_ecg_arr)+1, closed='left'))
                 .resample('8ms').max() )
    #recreated_ecg.plot(figsize=(30,4));

    data = recreated_ecg_arr[55:]  # remove first part of ECG (calibration?)
    sample_rate = len(data) / 30  # rough estimate of "sample rate" (based on number of pixels in png...)

    resampled_data = resample(data, len(data) * 4)  # heartpy recommends trying different resampling rates
    wd = None
    m = None
    try:
      wd, m = hp.process(hp.scale_data(resampled_data), sample_rate * 4)
      valid = True
    except:
      valid = False


    #plt.figure(figsize=(16,4))
    #hp.plotter(wd, m)

    #display(pd.DataFrame([m]))

    #hp.plot_poincare(wd, m)

    filtered = hp.filter_signal(data, cutoff = 0.05, sample_rate = sample_rate, order=2, filtertype='notch')

    #plt.figure(figsize=(20,4))
    #plt.plot(data, alpha=0.7, label = 'original signal')
    #plt.plot(filtered, alpha=0.7, label = 'filtered signal')
    #plt.legend()

    resampled_data = resample(filtered, len(filtered) * 2)
    try:
      wd, m = hp.process(hp.scale_data(resampled_data), sample_rate * 2)
      valid = True
    except:
      valid = False

    #plt.figure(figsize=(16,4))
    #hp.plotter(wd, m)
    if valid is True:
      m["skew"] = skew_
      m["kurt"] = kurt
      m["avrr"] = avrr
      m["sdrr"] = sdrr
      m["csi"] = csi
      m["cvi"] = cvi
      m["modifiedcvi"] = modifiedcvi
      #m["meanrr"] = meanrr
      #m["medianrr"] = medianrr
      #m["hr"] = hr
      m["sdrr_rmssd"] = sdrr_rmssd
      m["emotion"] = emotion
      #m["file"] = name
      rows.append(m)

row_df = pd.DataFrame(rows)
##row_df = row_df * 0.10
row_df = row_df.dropna()
row_df

filepath = Path('csv/ecg_game.csv')
filepath.parent.mkdir(parents=True, exist_ok=True)
row_df.to_csv(filepath)